{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>VC07: Biclustering</h1></center>\n",
    "\n",
    "En esta práctica estudiaremos el funcionamiento y la utilización de diferentes algoritmos de biclustering ya implementados en diferentes librerías. Por un lado, conoceremos la librería <b>biclustlib</b>, que es necesario instalar a partir del siguiente repositorio:\n",
    "\n",
    "https://github.com/padilha/biclustlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En la librería <b>biclustlib</b> encontramos una implementación del algoritmo de Cheng y Church. Además usaremos el conjunto de datos que viene con la librería relativo a la expresión genética de la levadura, de uso habitual en biotecnología.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biclustlib.algorithms import ChengChurchAlgorithm\n",
    "from biclustlib.datasets import load_yeast_tavazoie\n",
    "\n",
    "# Cargamos los datos de levadura ('yeast' en inglés)\n",
    "data = load_yeast_tavazoie().values\n",
    "\n",
    "# los valores perdidos se imputan de manera aleatoria \n",
    "# de acuerdo a la recomendación de Cheng y Church\n",
    "missing = np.where(data < 0.0)\n",
    "data[missing] = np.random.randint(low=0, high=800, size=len(missing[0]))\n",
    "\n",
    "# Esta parametrización específica es la que originalmente \n",
    "# describían Cheng y Church en su artículo\n",
    "modelo = ChengChurchAlgorithm(num_biclusters=100,\n",
    "                              msr_threshold=300.0, \n",
    "                              multiple_node_deletion_threshold=1.2)\n",
    "biclustering = modelo.run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una vez aprendido el modelo, podemos observar qué biclústeres se obtuvieron como resultado final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_bicluster = 0 # índice del biclúster a examinar\n",
    "\n",
    "print(biclustering.biclusters[ind_bicluster])\n",
    "print(\"El área que cubre este bicluster es:\",biclustering.biclusters[ind_bicluster].area, \n",
    "     \"sobre un total de\",np.product(data.shape))\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(data[np.ix_(biclustering.biclusters[ind_bicluster].rows,\n",
    "                        biclustering.biclusters[ind_bicluster].cols)], \n",
    "            cmap=plt.cm.Blues)\n",
    "plt.title(\"Bicluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "También podemos generar nuestros propios datos artificiales para estudiar el funcionamiento de los algoritmos usando la función <b>make_biclusters</b> de sklearn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_biclusters\n",
    "size = 300\n",
    "\n",
    "# Usamos la función make_biclusters para generar una matriz con biclusters aislados\n",
    "# (no todas las filas ni columnas pertenecen a algún clúster)\n",
    "n_clusters = 5\n",
    "orig_data, rows, columns = make_biclusters(shape=(size, size), n_clusters=n_clusters, \n",
    "                                           noise=10, shuffle=False, random_state=0)\n",
    "\n",
    "# Inducimos un desorden aleatorio de filas y de columnas\n",
    "sh_rows = np.random.choice(size, size=size, replace=False)\n",
    "sh_cols = np.random.choice(size, size=size, replace=False)\n",
    "transf_data = orig_data[sh_rows,:]\n",
    "transf_data = transf_data[:,sh_cols]\n",
    "\n",
    "\n",
    "# En esta ocasión estamos usando información privilegiada: sabemos cuántos clústeres \n",
    "# se generaron\n",
    "modelo = ChengChurchAlgorithm(num_biclusters=n_clusters, \n",
    "                              msr_threshold=200.0, \n",
    "                              multiple_node_deletion_threshold=1.5)\n",
    "biclustering = modelo.run(transf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Puedes variar los párametros del modelo, tanto <b>msr_threshold</b> como <b>multiple_node_deletion_threshold</b> para observar el comportamiento y el tipo de coagrupamientos que se obtienen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_bicluster = 0 # índice del biclúster a examinar\n",
    "\n",
    "print(biclustering.biclusters[ind_bicluster])\n",
    "print(\"El área que cubre este bicluster es:\",biclustering.biclusters[ind_bicluster].area, \n",
    "     \"sobre un total de\",np.product(transf_data.shape))\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(transf_data[np.ix_(biclustering.biclusters[ind_bicluster].rows,\n",
    "                        biclustering.biclusters[ind_bicluster].cols)], \n",
    "            cmap=plt.cm.Blues)\n",
    "plt.title(\"Dataset original\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Por su parte, la librería ScikitLearn implementa el algoritmo de coagrupamiento espectral. De la parte teórica de la asignatura sabemos que este tipo de algoritmo es especialmente útil cuando los clústeres tienen forma de tablero de ajedrez. \n",
    "\n",
    "Podemos generar datos artificiales de este estilo usando la función <b>make_checkerboard</b> de sklearn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_checkerboard, make_biclusters\n",
    "from sklearn.cluster.bicluster import SpectralBiclustering, SpectralCoclustering\n",
    "from sklearn.metrics import consensus_score\n",
    "\n",
    "np.random.seed(17)\n",
    "\n",
    "size = 300\n",
    "\n",
    "# Usamos la función make_checkerboard para generar una matriz con la forma de tablero\n",
    "# de ajedrez que, como sabemos, suele darse en escenarios de coagrupamiento\n",
    "n_clusters = (4, 3)\n",
    "orig_data, rows, columns = make_checkerboard(shape=(size, size), n_clusters=n_clusters, \n",
    "                                             noise=100, shuffle=False, random_state=0)\n",
    "# Usaríamos la función make_biclusters para generar una matriz con biclusters aislados\n",
    "# (no todas las filas ni columnas pertenecen a algún clúster)\n",
    "# n_clusters = 5\n",
    "# orig_data, rows, columns = make_biclusters(shape=(size, size), n_clusters=n_clusters, \n",
    "#                                            noise=10, shuffle=False, random_state=0)\n",
    "\n",
    "# Buscamos un desorden aleatorio de filas y de columnas\n",
    "sh_rows = np.random.choice(size, size=size, replace=False)\n",
    "sh_cols = np.random.choice(size, size=size, replace=False)\n",
    "transf_data = orig_data[sh_rows,:]\n",
    "transf_data = transf_data[:,sh_cols]\n",
    "\n",
    "# Realizamos el biclustering sobre los datos desordenados\n",
    "# Asumimos que conocemos el número de clústeres (por filas y columnas)\n",
    "modelo = SpectralBiclustering(n_clusters=n_clusters, method='log',\n",
    "                              random_state=0)\n",
    "# También podríamos usar el algoritmo de SpectralCoclustering\n",
    "# modelo = SpectralCoclustering(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "modelo.fit(transf_data)\n",
    "\n",
    "# Aprovechamos que estamos en un entorno simulado y conocemos los \n",
    "# biclústeres originales para comprobar el parecido de los biclústeres \n",
    "# obtenidos con respecto a los originales\n",
    "score = consensus_score(modelo.biclusters_,\n",
    "                        (rows[:, sh_rows], columns[:, sh_cols]))\n",
    "\n",
    "print(\"El parecido según la Medida de Consenso es: \", format(score))\n",
    "\n",
    "# Reconstruimos la matriz original según el agrupamiento por filas \n",
    "# y columnas devuelto por el modelo  \n",
    "fit_data = transf_data[np.argsort(modelo.row_labels_)]\n",
    "fit_data = fit_data[:, np.argsort(modelo.column_labels_)]\n",
    "\n",
    "struct_data = np.outer(np.sort(modelo.row_labels_) + 1,\n",
    "                       np.sort(modelo.column_labels_) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "También podemos dibujar la matriz de datos en las diferentes etapas para ver cómo se produce la reconstrucción de los datos:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(20,20))\n",
    "\n",
    "ax1.matshow(orig_data, cmap=plt.cm.Blues)\n",
    "ax1.set_title(\"Dataset original\")\n",
    "\n",
    "ax2.matshow(transf_data, cmap=plt.cm.Blues)\n",
    "ax2.set_title(\"Dataset desordenado\")\n",
    "\n",
    "ax3.matshow(fit_data, cmap=plt.cm.Blues)\n",
    "ax3.set_title(\"Reordenado según el resultado del biclustering\")\n",
    "\n",
    "ax4.matshow(struct_data, cmap=plt.cm.Blues)\n",
    "ax4.set_title(\"Estructura inferida del biclustering obtenido\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Puedes probar a usar diferente parámetros (introducir más o menos ruido, por ejemplo) para observar el comportamiento del algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "Por último en este tutorial, veremos otra aplicación práctica relacionada con el análisis de biclústeres. En este caso se trata de agrupar un conjunto de noticias y encontrar, a su vez, sobre qué hablan esos grupos de noticias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos a los datos\n",
    "categories = ['alt.atheism', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', \n",
    "              'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "y_true = newsgroups.target\n",
    "\n",
    "# Tokenizer que, además, reemplaza todos los tokens numéricos por una misma etiqueta\n",
    "def tokenizer(doc):\n",
    "    patrones = re.compile(u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "    tokens = patrones.findall(doc)\n",
    "    tokens = [\"#NUMBER\" if token[0] in \"0123456789_\" else token for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Obtenemos una matriz a partir de los documentos: un documento por fila\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=5, tokenizer=tokenizer)\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Llevamos a cabo el co-agrupamiento\n",
    "modelo_cocluster = SpectralCoclustering(n_clusters=len(categories), \n",
    "                                 svd_method='arpack', random_state=0)\n",
    "modelo_cocluster.fit(X)\n",
    "print(\"Información mutual (norm.) de SpectralCoclustering: \", \n",
    "      format(normalized_mutual_info_score(y_true, modelo_cocluster.row_labels_,\n",
    "                                          average_method='arithmetic')))\n",
    "\n",
    "# por comparación, aprendemos un k-means para observar si la consideración \n",
    "# de usar coagrupamiento en vez de un simple agrupamiento conduce a mejores resultados\n",
    "modelo_kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n",
    "                         random_state=0)\n",
    "y_kmeans = modelo_kmeans.fit_predict(X)\n",
    "print(\"Información mutual (norm.) de K-means: \", \n",
    "      format(normalized_mutual_info_score(y_true, y_kmeans,\n",
    "                                          average_method='arithmetic')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finalmente, podemos observar los tipos de artículos en los diferentes co-agrupamientos. Necesitaremos una serie de funciones auxiliares:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import iteritems\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def bicluster_ncut(i, cocluster):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    return sorted(iteritems(d), key=operator.itemgetter(1), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finalmente, mediante estas líneas mostraremos la lista de los mejores biclústeres con información sobre los temas principales que tratan los documentos de ese biclúster y las palabras más comunes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_ejemplos = 5 # Número de biclústeres a mostrar\n",
    "\n",
    "palabras = vectorizer.get_feature_names()\n",
    "documentos = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i, modelo_cocluster)\n",
    "                       for i in range(len(newsgroups.target_names)))\n",
    "mejores_biclusteres = np.argsort(bicluster_ncuts)[:mostrar_ejemplos]\n",
    "\n",
    "print()\n",
    "print(\"Los mejores biclústeres son:\")\n",
    "print()\n",
    "for ind, cluster in enumerate(mejores_biclusteres):\n",
    "    n_rows, n_cols = modelo_cocluster.get_shape(cluster)\n",
    "    bic_documentos, bic_palabras = modelo_cocluster.get_indices(cluster)\n",
    "\n",
    "    # categorias\n",
    "    counter = defaultdict(int)\n",
    "    for i in bic_documentos:\n",
    "        counter[documentos[i]] += 1\n",
    "    combinacion_categorias = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, nombre)\n",
    "                                       for nombre, c in most_common(counter)[:3])\n",
    "\n",
    "    # palabras\n",
    "    bic_documentos_out = np.where(modelo_cocluster.row_labels_ != cluster)[0]\n",
    "    X_palabras = X[:, bic_palabras]\n",
    "    score_palabras = np.array(X_palabras[bic_documentos, :].sum(axis=0) -\n",
    "                              X_palabras[bic_documentos_out, :].sum(axis=0))\n",
    "    score_palabras = score_palabras.ravel()\n",
    "    lista_palabras = list(palabras[bic_palabras[i]]\n",
    "                          for i in score_palabras.argsort()[:-11:-1])\n",
    "    lista_palabras = ', '.join(lista_palabras)\n",
    "\n",
    "    print(\"Biclúster\", ind, \"(\", n_rows, \"documentos,\", n_cols, \"palabras)\")\n",
    "    print(\" + Categorias:\", combinacion_categorias)\n",
    "    print(\" + Palabras  :\", lista_palabras)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
