{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>VC03: Clustering Jerárquico Divisivo</h1></center>\n",
    "\n",
    "En esta práctica estudiaremos el funcionamiento y la utilización del clústering jerárquico divisivo.\n",
    "\n",
    "Para empezar, cargamos las librerías que vamos a necesitar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cut_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para comenzar, cargamos el conjunto de datos con el que trabajaremos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
    "\n",
    "data_file_url = 'https://raw.githubusercontent.com/javier-sevilla/ANS/master/datasets/sinteticos/dataset_reducido.csv'\n",
    "Dx = np.array(pd.read_csv(data_file_url,header=0))\n",
    "Dx = Dx[ np.random.choice(np.arange(Dx.shape[0]), Dx.shape[0], replace=False) ,:]\n",
    "\n",
    "plt.scatter(Dx[:,0], Dx[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Es el mismo dataset que usamos en la práctica anterior. Lo volveremos a usar ahora para estudiar las diferentes variantes del clústering divisivo. Empezamos nuevamente calculando la matriz de distancias, para lo que necesitaremos  la función matriz_distancias desarrollada en prácticas anteriores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_euclidiana(x, y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "def matriz_distancias(X, distancia):\n",
    "    mD = np.zeros((X.shape[0],X.shape[0]))\n",
    "    for pair in it.product(np.arange(X.shape[0]), repeat=2):\n",
    "        mD[pair] = distancia(X[pair[0],:],X[pair[1],:])\n",
    "    return mD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Necesitamos una función que, dada una matriz de distancias, construya un clustering divisivo. Es decir, partiendo de un único clúster (con todos los ejemplos de entrenamiento), se van dividiendo en dos iterativamente hasta que finalmente todos los elementos tienen su propio clúster unitario. En cada paso, el clúster a dividir se escoge como aquél que maximiza la disimilitud intraclúster (que puede ser definida de acuerdo a diferentes criterios). El clúster escogido se divide de acuerdo a cierto procedimiento de separación.\n",
    "\n",
    "Sabemos que el número de divisiones es igual a n-1 (el número de ejemplos menos uno). En la siguiente función, vamos guardando en cada columna de la matriz mParticiones la partición en clústeres de los ejemplos: cada partición mParticiones[:,p], de longitud n, guarda el clúster al que pertenece el i-ésimo ejemplo en la posición mParticiones[i,p]. Para rellenar esa matriz, en cada paso (división), se calcula la disimilitud intraclúster de todos los clústeres usando el criterio elegido y se elige un clúster a dividir (iClusterADiv). Después, se aplica el procedimiento de separación elegido para obtener un subconjunto de elementos de ese clúster que pasan a formar parte de un nuevo clúster (se les asigna al clúster con índice n). Este procedimiento iterativo se repite hasta que sólo quedan clústeres unitarios (cada clúster tiene un único elemento) en el paso final:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clustering_divisivo(mD, disimilitud_intracluster, separador, **kwargs):\n",
    "    mParticiones = np.zeros(mD.shape[0]**2,dtype=np.int8)\n",
    "    mParticiones.shape = (mD.shape[0],Dx.shape[0])\n",
    "\n",
    "    for n in (np.arange(mD.shape[0]-1)+1):\n",
    "        mParticiones[:,n] = mParticiones[:,n-1]\n",
    "\n",
    "        # calcular la disimilitud intracluster\n",
    "        # y elegir el que tenga mas\n",
    "        rDisIntraCluster = disimilitud_intracluster(mD, mParticiones[:,n], n)\n",
    "        iClusterADiv = np.argmax(rDisIntraCluster)\n",
    "\n",
    "        iAClusterNuevo = separador(mD, mParticiones[:,n], iClusterADiv, **kwargs)\n",
    "\n",
    "        mParticiones[iAClusterNuevo,n] = n\n",
    "    return mParticiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como se comentaba anteriormente, la primera decisión a tomar es qué clúster se va a dividir. Para ello se evalúa la disimilitud intraclúster. Hay diferentes maneras de medir dicha disimilitud. Una de ellas es mediante el <b>diámetro</b> del clúster, que se define como la máxima distancia entre dos elementos cualquiera del clúster. La siguiente función calcula dicho valor (diámetro) para todos los clústeres y los devuelve en un array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disimilitud_diametro(mD, particion, n):\n",
    "    res = [ np.max((mD[ np.ix_(particion == pn , particion == pn) ] +\n",
    "                    np.transpose(mD[ np.ix_(particion == pn , particion == pn) ]))/ 2)\n",
    "           for pn in np.arange(n)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Otra manera de calcular la disimilitud intraclúster es usando la <b>disimilitud media</b> del clúster, que se define como la distancia media entre todos los pares de elementos del clúster. La siguiente función calcula dicha disimilitud para todos los clústeres y los devuelve en un array: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disimilitud_media(mD, particion, n):\n",
    "    res = [np.sum(mD[ np.ix_(particion == pn , particion == pn) ])\n",
    "                              / (np.sum(particion == pn)**2)\n",
    "                              for pn in np.arange(n)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una vez identificado el clúster que queremos dividir, el siguiente objetivo es saber cómo lo vamos a dividir. Para ello, se pueden aplicar también diferentes procedimientos.\n",
    "\n",
    "El procedimiento propuesto por Macnaughton y Smith consiste en identificar, inicialmente, el ejemplo del clúster con mayor diferencia media con respecto al resto de elementos del clúster. Éste será el primer ejemplo que se separa del clúster. A partir se ese momento, de manera iterativa, se van separando nuevos elementos del primer clúster (original) y se incluyen en el nuevo grupo. En cada paso, el ejemplo que se traspasa es aquel que tiene menor distancia media con los elementos del nuevo clúster con respecto a la distancia media con los otros elementos que todavía están en el clúster original. Este traspaso se repite hasta que no haya ningún elemento en el clúster original que esté más cerca (en media) a los elementos del otro clúster que a los del suyo propio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separacion_macnaughton_smith(mDglobal, particion, iCluster):\n",
    "    indsClust = np.where(particion==iCluster)[0]\n",
    "    mD = mDglobal[ np.ix_(indsClust , indsClust)]\n",
    "    \n",
    "    SA = np.array([np.argmax(1/((mD.shape[0]-1)*2)*(np.sum(mD,axis=0)+np.sum(mD,axis=1)))])\n",
    "    SB = np.delete(np.arange(mD.shape[0]), SA)\n",
    "    if (SB.size == 1):\n",
    "        return indsClust[SA]\n",
    "\n",
    "    while True:\n",
    "        res = np.zeros(SB.size)\n",
    "        # Para todos los elementos del cluster original SB\n",
    "        for i in np.arange(SB.size):\n",
    "            SBa = np.delete(SB,i) \n",
    "            res[i] = (# distancia media con los otros elementos de SB\n",
    "                np.sum(mD[ SB[i] , SBa ]) + \n",
    "                np.sum(mD[ SBa , SB[i] ])\n",
    "            ) / (2 * SBa.size) - (# distancia media con los elementos ya movidos a SA\n",
    "                np.sum(mD[ SB[i] , SA ]) +\n",
    "                np.sum(mD[ SA , SB[i] ])\n",
    "            ) / (2 * SA.size)\n",
    "\n",
    "\n",
    "        # Si en todos los casos (todas las posiciones del vector res) la distancia \n",
    "        # es negativa quiere decir que no existe ningún elemento en SB más cercano \n",
    "        # a SA que al resto de SB\n",
    "        if #### TU CODIGO AQUI ####\n",
    "            break\n",
    "\n",
    "        iToChange = np.argmax(res)\n",
    "        SA = np.sort(np.append(SA, SB[iToChange]))\n",
    "        SB = np.delete(np.arange(mD.shape[0]), SA)\n",
    "\n",
    "    # Devolveremos los índices de los elementos que pertenecerán \n",
    "    # al clúster SA\n",
    "    return indsClust[SA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Otra manera de realizar la separación del clúster en dos es mediante el algoritmo K-means con K=2. Así se obtienen dos clústeres (a partir del clúster original) de manera rápida y utilizando éste popular algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separacion_Kmeans(mDglobal, particion, iCluster, Dxglobal):\n",
    "    indsClust = np.where(particion==iCluster)[0]\n",
    "    Dx = Dxglobal[indsClust,:]\n",
    "    \n",
    "    modelo = KMeans(n_clusters=#### TU CODIGO AQUI ####)\n",
    "    modelo = modelo.fit(Dx)\n",
    "    Dyp = modelo.predict(Dx)\n",
    "\n",
    "    # Devolveremos los índices de los ejemplos asignados \n",
    "    # al segundo clúster\n",
    "    return indsClust[Dyp==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finalmente, incluimos las funciones que nos permiten visualizar los resultados:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra los ejemplos coloreados según su pertenencia a los clústeres\n",
    "# K indica el número de clústeres a mostrar a partir de la jerarquía\n",
    "def plot_particion_K(Dx, mParticiones,K):\n",
    "    cmap = get_cmap('tab20')\n",
    "    vals = np.arange(Dx.shape[0]+2)/(Dx.shape[0]+2)\n",
    "    rgba = cmap(vals[np.arange(Dx.shape[0])+1])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.scatter(Dx[:,0],Dx[:,1], c=rgba[mParticiones[:,K-1],:])\n",
    "\n",
    "# Muestra el plot de todas las particiones posibles de la jerarquía\n",
    "def plot_particiones(Dx, mParticiones):\n",
    "    for k in np.arange(mParticiones.shape[1]):\n",
    "        plot_particion_K(Dx, mParticiones,k+1)\n",
    "\n",
    "# Muestra el dendrograma de la jerarquía\n",
    "def plot_dendrograma_de_mparticiones(mD, mParticiones):\n",
    "    nPasos = mD.shape[0]-1\n",
    "    distancias = np.zeros(nPasos)\n",
    "    tamanyos = np.zeros(nPasos)\n",
    "    uniones = np.zeros(2*nPasos,dtype=np.int8)\n",
    "    uniones.shape=(nPasos,2)\n",
    "\n",
    "    mNParticiones = mParticiones.copy()\n",
    "    for n in np.arange(mNParticiones.shape[0]):\n",
    "        valor = mParticiones[n,mNParticiones.shape[1]-1]\n",
    "        mNParticiones[mParticiones==valor] = n\n",
    "\n",
    "    k = 0\n",
    "    aux = np.array(range(mNParticiones.shape[1]-1))\n",
    "    for n in aux[::-1]:\n",
    "        # cual es el diferente?\n",
    "        prim_diferencia = np.where(mParticiones[:,n]!=mParticiones[:,n+1])[0][0]\n",
    "        submatriz = mNParticiones[:,:n+1]\n",
    "        submatriz[submatriz==mNParticiones[prim_diferencia,n]] = mNParticiones.shape[1]+k\n",
    "        mNParticiones[:,:n+1] = submatriz\n",
    "        uniones[k,:] = np.unique(mNParticiones[mNParticiones[:,n]==mNParticiones.shape[1]+k,n+1])\n",
    "        tamanyos[k] = np.sum(mNParticiones[:,n]==mNParticiones.shape[1]+k)\n",
    "\n",
    "        vs = mNParticiones[:,n+1]\n",
    "        distancias[k] = 1 / (2 * tamanyos[k]) * (    \n",
    "            np.sum(mD[ np.ix_(np.where(vs == uniones[k,0])[0],\n",
    "                              np.where(vs == uniones[k,1])[0])])+\n",
    "            np.sum(mD[ np.ix_(np.where(vs == uniones[k,1])[0],\n",
    "                              np.where(vs == uniones[k,0])[0])]))\n",
    "        k += 1\n",
    "\n",
    "    distancias = np.arange(uniones.shape[0])\n",
    "    # Creamos la matriz de enlaces que necesita el método dendrogram de scipy\n",
    "    mEnlaces = np.column_stack([uniones, distancias, tamanyos]).astype(float)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.title('Dendrograma de Clustering Jerárquico')\n",
    "    plt.xlabel('Índice del caso')\n",
    "    plt.ylabel('Distancia')\n",
    "    dendrogram(mEnlaces)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora ya estamos en disposición de realizar el clustering jerárquico divisivo. Empezaremos por calcular la matriz de distancias y, por este orden, calcularemos y mostraremos el dendrograma de los diferentes agrupamientos obtenidos de usar el criterio de disimilitud media y diámetro, en combinación con la separación de Macnaughton-Smith y la de K-means. También mostraremos (plotear) las respectivas particiones con 5 clústeres: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mD = matriz_distancias(Dx, distancia_euclidiana)\n",
    "K = 5 # parametro para mostrar una particion\n",
    "\n",
    "# Usando disimilitud media y el método de separacion de Macnaughton-Smith\n",
    "mParticiones = clustering_divisivo(mD, disimilitud_media, separacion_macnaughton_smith)\n",
    "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
    "plot_particion_K(Dx, mParticiones, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando disimilitud diámetro y el método de separacion de Macnaughton-Smith\n",
    "mParticiones = clustering_divisivo(mD, disimilitud_diametro, separacion_macnaughton_smith)\n",
    "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
    "plot_particion_K(Dx, mParticiones, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando disimilitud media y K-means como método de separacion\n",
    "mParticiones = clustering_divisivo(mD, disimilitud_media, separacion_Kmeans, Dxglobal=Dx)\n",
    "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
    "plot_particion_K(Dx, mParticiones, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Usando disimilitud diámetro y K-means como método de separacion\n",
    "mParticiones = clustering_divisivo(mD, disimilitud_diametro, separacion_Kmeans, Dxglobal=Dx)\n",
    "plot_dendrograma_de_mparticiones(mD, mParticiones)\n",
    "plot_particion_K(Dx, mParticiones, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2>Implementaciones en librerías de Python</h2>\n",
    "\n",
    "Dada la complejidad añadida del clustering jerárquico divisivo (respecto al aglomerativo), las principales librarías de aprendizaje automático de Python <b>no implementan</b> esta versión del agrupamiento jerárquico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
