{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>VC04: Agrupamiento espectral, conceptos básicos</h1></center>\n",
    "\n",
    "En esta práctica estudiaremos las ideas básicas que hemos visto como introducción al agrupamiento espectral.\n",
    "\n",
    "Para empezar, cargamos las librerías que vamos a necesitar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para comenzar, cargamos el conjunto de datos con el que trabajaremos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(17) # Fijamos una semilla para asegurar la reproducibilidad de la práctica\n",
    "\n",
    "data_file_url = 'https://raw.githubusercontent.com/javier-sevilla/ANS/master/datasets/sinteticos/dataset_reducido.csv'\n",
    "#data_file_url = 'https://raw.githubusercontent.com/javier-sevilla/ANS/master/datasets/sinteticos/dataset_circulos_concentricos.csv'\n",
    "#data_file_url = 'https://raw.githubusercontent.com/javier-sevilla/ANS/master/datasets/sinteticos/dataset_cuatro_diferente_medida.csv'\n",
    "#data_file_url = 'https://raw.githubusercontent.com/javier-sevilla/ANS/master/datasets/sinteticos/dataset_inseparable.csv'\n",
    "#data_file_url = 'https://raw.githubusercontent.com/javier-sevilla/ANS/master/datasets/sinteticos/dataset_dos_remolinos.csv'\n",
    "D = np.array(pd.read_csv(data_file_url,header=0))\n",
    "D = D[ np.random.choice(np.arange(D.shape[0]), D.shape[0], replace=False),:]\n",
    "Dx = D[:,:2]\n",
    "#Dy = D[:,2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(Dx[:,0], Dx[:,1])#, c = Dy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Antes de comenzar, cargamos la función que nos permite dibujar el grafo de afinidad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_grafo_afinidad(Dx, A):\n",
    "    minVal = np.min(A[np.nonzero(A)])\n",
    "    aux = (A-minVal)/(np.max(A)-minVal)\n",
    "    W = np.zeros(A.shape)\n",
    "    W[np.nonzero(A)] = 5*(aux[np.nonzero(A)]+.1)\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.scatter(Dx[:,0],Dx[:,1])\n",
    "    inds = np.where(A>0)\n",
    "    for i in np.arange(1,inds[0].size):\n",
    "        ax.plot([Dx[inds[0][i],0],Dx[inds[1][i],0]],[Dx[inds[0][i],1],Dx[inds[1][i],1]], \n",
    "                linestyle='-', linewidth=W[inds[0][i],inds[1][i]],c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En agrupamiento espectral trabajamos con la matriz de similitudes. Dada la matriz de distancias (euclidiana) que obtenemos con la funcion euclidean_distances de la librería scikit-learn, se calcula la matriz de similitud de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "mSimilitud = euclidean_distances(Dx)\n",
    "mSimilitud = np.exp(-np.power(mSimilitud,2)/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Se puede construir un grafo de afinidad a partir de una matriz de similitud de diferentes maneras. Una de ellas es mediante el procedimiento de los vecinos más cercanos. Cada ejemplo se enlaza con sus $K$ vecinos más cercanos. A la hora de darle peso a cada arista, existen también diferentes apreciaciones. Una de las más sencillas sería asignar peso $1$ a la arista entre dos nodos $(e,f)$ si $e$ es uno de los $K$ vecinos más cercanos de $f$ y $f$ lo es de $e$. Se asignaría peso $0.5$ si sólo se cumpliese en un sentido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KNN = 4\n",
    "\n",
    "# No queremos que un nodo sea vecino más cercano de si mismo\n",
    "np.fill_diagonal(mSimilitud, 0)\n",
    "\n",
    "W = np.zeros(mSimilitud.shape)\n",
    "a = (np.argsort(-mSimilitud, axis=0)[0:KNN,:]).flatten()\n",
    "b = np.tile(np.arange(mSimilitud.shape[0]),KNN)\n",
    "W[a,b] = 1\n",
    "\n",
    "a = np.repeat(np.arange(mSimilitud.shape[0]),KNN)\n",
    "b = (np.argsort(-mSimilitud, axis=1)[:,0:KNN]).flatten()\n",
    "W[a,b] += 1\n",
    "\n",
    "W /= 2\n",
    "\n",
    "plt_grafo_afinidad(Dx, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En la parte de teoría afirmábamos que un algoritmo de agrupamiento sobre un conjunto de datos puede verse como un algoritmo de corte sobre el grafo de afinidad correspondiente.\n",
    "\n",
    "Para explicar qué es un algoritmo de corte es necesario explicar la función de corte. Ésta es una función que se define sobre un grafo y dos subgrupos complementarios de los nodos del grafo. La función corte cuenta el número de aristas que unen ambos subconjuntos de nodos. Si las aristas tienen un peso asignado, la función se define como la suma de los pesos de las aristas que unen ambos subconjuntos de nodos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corte(W, A, B):\n",
    "    return np.sum([[W[i,j] for j in B] for i in A])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Un algoritmo de corte busca en un grafo la separación en dos que minimiza la función de corte. Aunque existen aproximaciones eficientes (ver Algoritmo de Karger, https://en.wikipedia.org/wiki/Karger%27s_algorithm), la búsqueda del mejor corte por fuerza bruta se convierte rápidamente en inabarcable. Por eso, la limitamos a un número máximo de cortes 'stop_at':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minCorte = np.inf\n",
    "minAB = []\n",
    "resCorteNorm = np.array([])\n",
    "stop_at = 10000\n",
    "for r in np.arange(1,W.shape[0]):\n",
    "    for A in it.combinations(np.arange(W.shape[0]), r):        \n",
    "        B = set(np.arange(W.shape[0]))-set(A)\n",
    "        actCorte = corte(W, A, B)\n",
    "        stop_at -= 1\n",
    "        if actCorte < minCorte:\n",
    "            minCorte = actCorte\n",
    "            minAB = [set(A),B]\n",
    "        if stop_at <= 0.:\n",
    "            break\n",
    "\n",
    "print('El valor de corte mínimo es: ', minCorte)\n",
    "print('La separación que obtuvo corte mínimo es: ', minAB)\n",
    "\n",
    "colores = np.zeros(Dx.shape[0])\n",
    "colores[list(minAB[0])] = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(Dx[:,0], Dx[:,1], c = colores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sin embargo, como se puede ver en el resultado anterior, por definición de la función de corte, este algoritmo básico tiende a devolver particiones extremas (ej., sólo un nodo en una de las particiones). Es por ello que en la práctica suele usarse la función de corte normalizado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grado(W, i):\n",
    "    return np.sum(W[i,:])\n",
    "\n",
    "def corte_normalizado(W, G, A, B):\n",
    "    normal = 1. / np.sum([G[i] for i in A]) + 1. / np.sum([G[j] for j in B])\n",
    "    \n",
    "    return normal * corte(W, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Se puede volver a hacer la prueba de la búsqueda del mejor corte por fuerza bruta (limitada a un número máximo de cortes 'stop_at'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([grado(W,i) for i in np.arange(W.shape[0])])\n",
    "\n",
    "minCorte = np.inf\n",
    "minAB = []\n",
    "resCorteNorm = np.array([])\n",
    "stop_at = 10000\n",
    "for r in np.arange(1,W.shape[0]):\n",
    "    for A in it.combinations(np.arange(W.shape[0]), r):        \n",
    "        B = set(np.arange(W.shape[0]))-set(A)\n",
    "        # Una vez tenemos un posible corte A-B, calculamos el valor de corte normalizado\n",
    "        actCorte = corte_normalizado(W, D, A, B)\n",
    "        stop_at -= 1\n",
    "        if actCorte < minCorte:\n",
    "            minCorte = actCorte\n",
    "            minAB = [set(A),B]\n",
    "        if stop_at <= 0.:\n",
    "            break\n",
    "\n",
    "print('El valor de corte mínimo es: ', minCorte)\n",
    "\n",
    "print('La separación que obtuvo corte mínimo es: ', minAB)\n",
    "\n",
    "colores = np.zeros(Dx.shape[0])\n",
    "colores[list(minAB[0])] = 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(Dx[:,0], Dx[:,1], c = colores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como puede observarse, el tamaño de las particiones está más compensado al usar la medida de corte normalizado ya que se tiene en cuenta el grado medio de los elementos de ambas particiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "En la teoría hablábamos también de la <b>matriz de transiciones</b> (probabilidad de saltar de un nodo a otro) en uno y en varios saltos. La multiplicación de la matriz de transición por sí misma $n$ veces devuelve la probabilidad de saltar de un nodo a otro en $n$ pasos. \n",
    "\n",
    "En primer lugar, definiremos la matriz de transiciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mTrans = np.transpose(np.transpose(W)/np.sum(W,axis=0))\n",
    "print('La matriz de transiciones para los dos primeros ejemplos:\\n',mTrans[:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A continuación, calcularemos la matriz de transiciones en $n$ pasos. En este ejemplo, vamos a usar $n = 10$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mNTrans = mTrans.copy()\n",
    "n = 10\n",
    "for r in np.arange(n):\n",
    "    mNTrans = mNTrans.dot(mTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Podemos observar la probabilidad de acceder desde cada nodo al resto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.arange(Dx.shape[0]):\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.plot(Dx[i,0], Dx[i,1], marker='*', markersize=10, color = 'y')\n",
    "    ax.scatter(Dx[:,0], Dx[:,1], s = 200 * mNTrans[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "¡Prueba con diferentes valores de $n$!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}